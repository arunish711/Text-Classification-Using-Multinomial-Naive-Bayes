{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "vC7M3Zu1iamV",
    "outputId": "faa07aac-3467-4377-d3c1-8266378e52b2"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "import math \n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dT461hOoiamf"
   },
   "outputs": [],
   "source": [
    "news=datasets.fetch_20newsgroups()   \n",
    "x=news.data\n",
    "y=news.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ViEuP0KWiamh"
   },
   "outputs": [],
   "source": [
    "  #doing spliting for train and test data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oryq73c9iaml"
   },
   "outputs": [],
   "source": [
    "newsgroup_train_data,newsgroup_test_data,newsgroup_train_target,newsgroup_test_target=model_selection.train_test_split(x,y,test_size=0.25,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the both training and testing documents converted into tuple form in which first element consists of data and\n",
    "## second element consists it's class to which it belongs.After that we are shuflling the training documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3BL6Bntoiamo"
   },
   "outputs": [],
   "source": [
    "traindocuments=[]\n",
    "testdocuments=[]\n",
    "traindocuments=[(newsgroup_train_data[i],newsgroup_train_target[i]) for i in range(0,len(newsgroup_train_target))]\n",
    "testdocuments=[(newsgroup_test_data[i],newsgroup_test_target[i]) for i in range(0,len(newsgroup_test_target))]\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting the training and testing documents into word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "foEr-seDiamr"
   },
   "outputs": [],
   "source": [
    "train_documents=[(word_tokenize(document),category) for document,category in traindocuments]\n",
    "test_documents=[(word_tokenize(document),category) for document,category in testdocuments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_VQC2ttiamu"
   },
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G8kRDeRkiamx"
   },
   "outputs": [],
   "source": [
    "def get_simple_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BJMBDPssiam1"
   },
   "outputs": [],
   "source": [
    "stops=set(stopwords.words('english'))  ## Creating List of words that we don't want in training as well as testing documents.\n",
    "punctuations=list(string.punctuation)  ## Also we also need to remove various punctuations from our data.\n",
    "stops.update(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zgjCoDm8iam3"
   },
   "outputs": [],
   "source": [
    "def clean_text(words):    ## Cleaning the documents.\n",
    "    output_words=[]\n",
    "    \n",
    "    for w in words:\n",
    "        if w.lower() not in stops:\n",
    "            pos=pos_tag([w])\n",
    "            clean_word=lemmatizer.lemmatize(w,pos=get_simple_pos(pos[0][1]))\n",
    "            output_words.append(clean_word.lower())\n",
    "    return output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we have our cleaned training as well as testing documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-BoRHL0-iam5"
   },
   "outputs": [],
   "source": [
    "train_documents=[(clean_text(document),category) for document,category in train_documents]\n",
    "test_documents=[(clean_text(document),category) for document,category in test_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we are getting our features on which we will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CaGBitRciam_"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=3000)\n",
    "t_train=[\" \".join(document) for document,category in train_documents]\n",
    "t_test=[\" \".join(document) for document,category in test_documents]\n",
    "t_train=vectorizer.fit_transform(t_train)\n",
    "features=vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we are calculating the probability of data point for a particular class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xKZ2AWjHianC"
   },
   "outputs": [],
   "source": [
    "def probability(dictionary,x,current_class,features,distinct_words):\n",
    "    output=(dictionary[current_class][\"total_count\"])/(dictionary[\"total_data\"])\n",
    "    output=math.log(output)\n",
    "    num_features=len(dictionary[current_class].keys())-2\n",
    "    \n",
    "    for j in range(2,num_features+2):\n",
    "        if(x[j-2]==0):\n",
    "            continue\n",
    "        word_jcnt=dictionary[current_class][features[j-2]]+1\n",
    "        total_word_cnt=dictionary[current_class][\"total_word_count\"]+distinct_words\n",
    "        prob_wordj=(word_jcnt)/(total_word_cnt)\n",
    "        prob_wordj=math.log(prob_wordj)\n",
    "        output=(output)+(prob_wordj)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function return the best_class i.e the class which has highest probability to contain a data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q1v44gcUianF"
   },
   "outputs": [],
   "source": [
    "def predictSinglePoint(dictionary,x,features,distinct_words):\n",
    "    classes=dictionary.keys()\n",
    "    first_run=True\n",
    "    best_p=-10000000\n",
    "    best_class=-1\n",
    "    \n",
    "    for current_class in classes:\n",
    "        if(current_class==\"total_data\"):\n",
    "            continue\n",
    "        p_current_class=probability(dictionary,x,current_class,features,distinct_words)\n",
    "        \n",
    "        if(p_current_class > best_p or first_run):\n",
    "            best_p=p_current_class\n",
    "            best_class=current_class\n",
    "            first_run=False\n",
    "            \n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We are storing the predictions and returing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yrM6jrr9ianI"
   },
   "outputs": [],
   "source": [
    "def predict(dictionary,x_test,features,distinct_words):\n",
    "    y_pred=[]\n",
    "    for i in range(0,x_test.shape[0],1):\n",
    "        x_class=predictSinglePoint(dictionary,x_test[i,:],features,distinct_words)\n",
    "        y_pred.append(x_class)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we are training the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kHXBTmQ2ianK"
   },
   "outputs": [],
   "source": [
    "def fit(x_train,y_train,features):\n",
    "    result={} \n",
    "    class_values=set(y_train)\n",
    "    \n",
    "    for current_class in class_values:\n",
    "        result[\"total_data\"]=len(y_train) \n",
    "        result[current_class]={}\n",
    "        current_class_rows=(y_train==current_class)\n",
    "        x_train_current=x_train[current_class_rows]\n",
    "        y_train_current=y_train[current_class_rows]\n",
    "        num_features=x_train.shape[1]\n",
    "        total_word_cnt=0\n",
    "        result[current_class][\"total_count\"]=len(y_train_current)\n",
    "        result[current_class][\"total_word_count\"]=total_word_cnt\n",
    "        \n",
    "        for j in range(2,num_features+2):\n",
    "            result[current_class][features[j-2]]=x_train_current[:,j-2].sum()\n",
    "            total_word_cnt+=x_train_current[:,j-2].sum()\n",
    "          ##  if(x_train_current[:,j-2].sum()==0):\n",
    "          ##      continue\n",
    "           ## total_word_cnt+=1 ## Adding +1 for getting the count of the words for lapalace correction\n",
    "                                                                                          \n",
    "            \n",
    "        result[current_class][\"total_word_count\"]=total_word_cnt \n",
    "                                                                   \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we are getting our dataset ready of the form that is required for the Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aiRms7FhianN"
   },
   "outputs": [],
   "source": [
    "check={}\n",
    "index={}\n",
    "distinct_words=0\n",
    "visited={}\n",
    "x_train=[[0 for i in range(len(features))] for j in range(len(newsgroup_train_target))]\n",
    "x_test=[[0 for i in range(len(features))] for j in range(len(newsgroup_test_target))]\n",
    "ind=0\n",
    "for word in features:\n",
    "    check[word]=1\n",
    "    index[word]=ind\n",
    "    ind+=1\n",
    "    \n",
    "row=0\n",
    "for document,category in train_documents:\n",
    "    col=0\n",
    "    for word in document:\n",
    "        if word in check:\n",
    "            x_train[row][index[word]]+=1\n",
    "            if word not in visited:\n",
    "                distinct_words+=1\n",
    "                visited[word]=1;\n",
    "    row+=1 \n",
    "\n",
    "row=0\n",
    "for document,category in test_documents:\n",
    "    col=0\n",
    "    for word in document:\n",
    "        if word in check:\n",
    "            x_test[row][index[word]]+=1\n",
    "    row+=1\n",
    "x_train=np.array(x_train)\n",
    "x_test=np.array(x_test)\n",
    "y_train=newsgroup_train_target\n",
    "y_test=newsgroup_test_target\n",
    "y_train=np.array(y_train)\n",
    "y_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finally! we have our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "L2Sx9sT7ianP",
    "outputId": "04814f15-5ac3-480f-f64a-5a9636dd8968"
   },
   "outputs": [],
   "source": [
    "dictionary=fit(x_train,y_train,features)\n",
    "y_pred=predict(dictionary,x_test,features,distinct_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now for the subsequent code we are predicting the dataset with the Sklearn's Multinomial Naive Bayes and\n",
    "## then we are comparing with our own implemented Naive Bayes through classification reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_classifier=clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing the classification_report of Our Own Implemented Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.80       112\n",
      "           1       0.52      0.80      0.63       147\n",
      "           2       0.92      0.16      0.28       140\n",
      "           3       0.57      0.72      0.64       148\n",
      "           4       0.66      0.81      0.72       149\n",
      "           5       0.83      0.74      0.78       159\n",
      "           6       0.67      0.85      0.75       131\n",
      "           7       0.81      0.79      0.80       158\n",
      "           8       0.81      0.91      0.86       162\n",
      "           9       0.90      0.93      0.91       148\n",
      "          10       0.96      0.88      0.92       150\n",
      "          11       0.98      0.89      0.93       155\n",
      "          12       0.81      0.71      0.76       147\n",
      "          13       0.91      0.89      0.90       131\n",
      "          14       0.92      0.84      0.88       154\n",
      "          15       0.88      0.83      0.85       155\n",
      "          16       0.87      0.92      0.90       144\n",
      "          17       0.92      0.85      0.89       144\n",
      "          18       0.82      0.81      0.82       108\n",
      "          19       0.76      0.67      0.71        87\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      2829\n",
      "   macro avg       0.81      0.79      0.79      2829\n",
      "weighted avg       0.82      0.79      0.79      2829\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing the classification report of Sklearn's Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.81       112\n",
      "           1       0.55      0.82      0.66       147\n",
      "           2       0.87      0.24      0.37       140\n",
      "           3       0.58      0.76      0.66       148\n",
      "           4       0.70      0.81      0.75       149\n",
      "           5       0.79      0.77      0.78       159\n",
      "           6       0.73      0.83      0.78       131\n",
      "           7       0.83      0.79      0.81       158\n",
      "           8       0.82      0.90      0.86       162\n",
      "           9       0.90      0.93      0.91       148\n",
      "          10       0.97      0.94      0.95       150\n",
      "          11       0.99      0.88      0.93       155\n",
      "          12       0.79      0.73      0.76       147\n",
      "          13       0.93      0.89      0.91       131\n",
      "          14       0.92      0.86      0.89       154\n",
      "          15       0.89      0.85      0.86       155\n",
      "          16       0.83      0.89      0.86       144\n",
      "          17       0.92      0.85      0.88       144\n",
      "          18       0.76      0.78      0.77       108\n",
      "          19       0.79      0.67      0.72        87\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      2829\n",
      "   macro avg       0.82      0.80      0.80      2829\n",
      "weighted avg       0.82      0.80      0.80      2829\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred_classifier))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Text_classification_data set.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
